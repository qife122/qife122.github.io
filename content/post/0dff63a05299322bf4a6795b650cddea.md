---
date: 2025-08-04T16:31:58+08:00
title: 开源AI威胁建模工具——快速、情境感知且开发者友好
tags: [AI安全, 威胁建模, 开源工具, 网络安全]
authors: qife
description: 本文介绍了一款专为AI系统设计的开源威胁建模工具，它通过结构化问卷帮助开发者识别模型反转、数据投毒等新型威胁，并映射到MITRE ATLAS框架，提供可操作的缓解建议。
---

# 开源AI威胁建模工具——快速、情境感知且开发者友好

人工智能正在变革各行各业，但随之而来的安全风险也日益复杂。传统安全方法（如STRIDE框架）难以应对AI特有的威胁场景，例如：
- **提示注入**：通过恶意输入操纵生成式AI行为
- **训练数据投毒**：污染数据集导致模型预测偏差
- **模型窃取**：通过API交互窃取模型参数
- **组件链风险**：向量数据库、API网关等组件的协同攻击面

## 工具核心能力
这款基于Streamlit构建的开源工具通过两阶段工作流实现威胁建模：

1. **交互式问卷**  
   收集AI系统关键信息：
   - 模型类型（分类/生成/集成）
   - 数据来源（内部/爬取/用户生成）
   - 部署模式（SaaS/混合云）
   - 第三方依赖（开源模型/向量数据库）

2. **规则引擎**  
   基于300+条确定性规则生成：
   - 映射到MITRE ATLAS的攻击战术
   - NIST CIA+Abuse分类
   - 合规缺口提示（GDPR/HIPAA）
   - 可操作的修复建议（如输入过滤、结构化提示）

## 技术实现
- **前端**：Streamlit构建的渐进式问卷
- **后端**：Python规则引擎，示例规则逻辑：
```json
{
  "match": ["unfiltered_user_input", "llm_used"],
  "threat": "用户输入导致的提示注入",
  "mitre_atlas": "Prompt Injection",
  "mitigations": ["输入消毒", "结构化提示"]
}
```

## 适用场景
- 开发中的AI系统架构评审
- 安全团队评估LLM应用风险
- 产品经理向管理层解释AI风险

项目已在GitHub开源，支持本地部署：
```bash
pip install -r requirements.txt
streamlit run main.py
```

未来版本计划集成OWASP LLM Top 10威胁图谱和团队协作功能。开发者可通过提交PR扩展威胁规则库，共同构建更安全的AI生态。